# Вопросы к рассказу о легенде

**ВАЖНО!** Абсолютно к каждому пункту учимся отвечать самостоятельно, **что и как вы делали руками** (отвечаем сразу **практикой** и **синктаксисом**) - это радикально повышает ваши шансы!

## Могут возникнуть в процессе рассказа о легенде (или после)

#### 0. Расскажите о вашем проекте и о последнем месте работы. Что сопровождали? Чем занимались? Почему уходите (говорят сейчас из МТС Банка, Сбербанка, Тинькофф Банка все бегут)?
   - Всё о вашем проекте и сопровождении [указано здесь](https://github.com/lamjob1993/linux-monitoring/blob/main/navigation/%D0%9B%D0%B5%D0%B3%D0%B5%D0%BD%D0%B4%D0%B0%20%D0%B8%D0%BD%D0%B6%D0%B5%D0%BD%D0%B5%D1%80%D0%B0.md).
   - Чем занимались? Указано ниже по списку (сопровождение, клиенты, регистрация инцидентов, мониторинг и.т.д).
   - Почему уходите? Подробно выводил таблицу по каверзным вопросам к собеседованию [вот здесь](https://t.me/c/2168307578/233/315), но универсальная причина ухода: это то что не дают развиваться в текущем отделе, хотите в DevOps-SRE, но слишком много решаете банальных инцидентов и заявок (ни в коем случае не затрагиваем вопрос денег).

#### 0.1. Расскажи про распорядок дня? Как проходил дейли? Какой график и сколько раз выгоняют в офис?
   - [Распорядок дня + дейлик (как в живую, так и в Zoom), daily stand-up, стендап, дейли](https://github.com/lamjob1993/linux-monitoring/blob/main/navigation/%D0%9B%D0%B5%D0%B3%D0%B5%D0%BD%D0%B4%D0%B0%20%D0%B8%D0%BD%D0%B6%D0%B5%D0%BD%D0%B5%D1%80%D0%B0.md#2-%D1%80%D0%B0%D1%81%D0%BF%D0%BE%D1%80%D1%8F%D0%B4%D0%BE%D0%BA-%D0%B4%D0%BD%D1%8F)
   - График 5/2 на гибриде, 50/50 дом/офис
   - А по какому адресу офис? - метро Тульская

#### 0.2. Кто ставил задачи и какой был тасктрекер?
   - Ставил тимлид или сениор на дейлике (бывало, что обсуждали задачи прямо на обеде).
   - [Тасктрекер Jira](https://img4.teletype.in/files/fb/10/fb106b79-ea04-459c-946d-f83ae26aab97.png), создавал таски и задачи по внедрению.

#### 0.3. Сколько человек в команде и с кем работали и за что отвечали вы?
   - [Ответ](https://github.com/lamjob1993/linux-monitoring/blob/main/navigation/%D0%9B%D0%B5%D0%B3%D0%B5%D0%BD%D0%B4%D0%B0%20%D0%B8%D0%BD%D0%B6%D0%B5%D0%BD%D0%B5%D1%80%D0%B0.md#%D0%B2-%D0%BD%D0%B0%D1%88%D0%B5%D0%BC-%D0%BE%D1%82%D0%B4%D0%B5%D0%BB%D0%B5-%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%D1%8E%D1%82)
   - Я сопровождал инциденты, сопровождал мониториг, писал конфиги по автоматизации (Ansible, Bash).
#### 1. Расскажите о вашем опыте работы с Linux-администраторами?
   - Я тесно взаимодействую с командой администраторов Linux (CentOS/Debian), 4 человека. Мы совместно решаем задачи по обновлению ОС и внедрениям. Например, недавно мы автоматизировали процесс обновления пакетов ОС через Ansible.

#### 1.1. А как внедрялись, что делали для этого? Как проходило внедрение? А кто проводил, кто был на внедрении?
   - Внедрение - это написание конфига, разворачивание ПО. Короче, что угодно, что вы делали руками с YAML, JSON, Docker Compose и т.д, то есть любой написанный конфиг, даже самый простой сойдёт за внедрение - запушили в Bitbucket, а дальше админы подхватили и раскатали в ПРОД.
   - Универсальный вариант, это сказать, что вы писали конфиги для: экспортеров или для Ansible и пушили в Bitbucket, далее регили задачу в Jira с описанием процесса и далее админы подхватывали - вот и всё внедрение на встрече в Zoom.
   - Собирались в комнате Zoom и раскатывали внедрение.
   - Проводил внедрение тимлид или сениор от нашей или соседней команды.
   - На внедрении была часть команды админов и часть команды от нашей по сопровождению.

#### 2. Как вы взаимодействуете с разработчиками?
   - Ежедневно работаю с разработчиками (20 человек Java/Kotlin из соседних команд) над решением инцидентов в production (ПРОД). Через CI/CD-пайплайны TeamCity мы внедряем новые релизы (внедряют админы, а вы как наблюдатель, но в свободное от работы время проходите стек самостоятельно). Также регулярно проводим постмортем-анализ инцидентов для предотвращения повторений, кстати, страничку постмортема в Confluence написал я (постмортем - это детальный анализ инцидента или сбоя в системе, продукте или проекте).

#### 3. Опишите опыт работы с аналитиками?
   - Для аналитиков я создаю специальные дашборды в Grafana (знаю двух аналитиков, общался только сними), включающие ключевые метрики производительности системы. Мы вместе анализируем тренды использования ресурсов и время реакции на инциденты. Например, благодаря таким анализам мы оптимизировали использование RAM.

#### 3.1. Как проходит взаимодействие со смежными командами?
   - Участвую в еженедельных встречах, где координирую действия между командами DevOps, разработчиков и администраторов. Внедрили общую систему мониторинга через Prometheus, которая позволяет всем командам видеть актуальное состояние инфраструктуры. При аварийных ситуациях организуем Zoom встречи для быстрого решения проблем, а также участвую по вторникам на еженедельной встрече по презентации мониторинга нашей платформы.
   - Мы разрабатываем мониторинг для команд от дашбордов Grafana до оптимизации метрик Prometheus. Встреча проходит для команд нашего департамента. Я презентую в рельном времени, как работает связка мониторинга Grafana + Prometheus + Node Exporter, как работают метрики, пишу простые формулы на языке PromQL, после чего мне задают вопросы. Заинтересованные команды могут заказать у нас мониторинг через заявку в Jira."

#### 4. Расскажите про стек отдела?
   - [Ответ](https://github.com/lamjob1993/linux-monitoring/blob/main/navigation/%D0%A1%D1%82%D0%B5%D0%BA%20%D0%BE%D1%82%D0%B4%D0%B5%D0%BB%D0%B0.md)

#### 4.1. Что такое Prometheus Server? Как работает и какие у него схемы сбора данных?
   - [Ответ](https://github.com/lamjob1993/linux-monitoring/blob/main/tasks/prometheus/beginning/1.%20%D0%92%D0%B2%D0%B5%D0%B4%D0%B5%D0%BD%D0%B8%D0%B5%20(%D0%9E%D1%81%D0%BD%D0%BE%D0%B2%D1%8B%20Prometheus).md)
   - [Схемы сбора данных](https://github.com/lamjob1993/linux-monitoring/blob/main/tasks/prometheus/beginning/1.%20%D0%92%D0%B2%D0%B5%D0%B4%D0%B5%D0%BD%D0%B8%D0%B5%20(%D0%9E%D1%81%D0%BD%D0%BE%D0%B2%D1%8B%20Prometheus).md#%D1%81%D0%B8%D1%81%D1%82%D0%B5%D0%BC%D1%8B-%D1%81%D0%B1%D0%BE%D1%80%D0%B0-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85-%D1%81%D1%80%D0%B0%D0%B2%D0%BD%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D1%8B%D0%B9-%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7-pull-%D0%B8-push-%D0%BF%D0%BE%D0%B4%D1%85%D0%BE%D0%B4%D0%BE%D0%B2)

#### 4.2. Что такое лейблы и таргеты? А как сделать группировку по лейблам?
- [Лейблы](https://github.com/lamjob1993/linux-monitoring/blob/main/tasks/prometheus/beginning/3.%20%D0%9B%D0%B5%D0%B9%D0%B1%D0%BB%D1%8B%20(Labels)%20%D0%B8%D0%BB%D0%B8%20%D0%BC%D0%B5%D1%82%D0%BA%D0%B8.md)
- [Таргеты](https://github.com/lamjob1993/linux-monitoring/blob/main/tasks/prometheus/beginning/3.1%20%D0%A2%D0%B0%D1%80%D0%B3%D0%B5%D1%82%D1%8B%20(Targets).md)
- Группировка: `group by (status, instance) (name_metric)`

#### 4.3. А как вывести [монотонно возрастающий счетчик](https://github.com/lamjob1993/linux-monitoring/blob/main/tasks/prometheus/beginning/6.%20PromQL%20(%D0%AF%D0%B7%D1%8B%D0%BA%20%D0%B7%D0%B0%D0%BF%D1%80%D0%BE%D1%81%D0%BE%D0%B2%20Prometheus).md)?
   - `rate(name_metric{}[1m])` или `irate(name_metric{}[1m])`
      - А как вывести сумму ошибок?
         - `sum (name_metric)`
      - А как группировку по суммам всех ошибок?
         - `sum by (status) (name_metric)`
      - А как вывести: min, max, avg?
         - `...`

#### 4.4. Как построишь мониторинг на базе Prometheus?
   - [Ответ](https://github.com/lamjob1993/linux-monitoring/tree/main/navigation/introduction_monitoring)

#### 5. Куда смотришь в Grafana (на какие имменно дашборды)?
   - Смотрю на дашборды:
      - Nginx (проверяю статус нашего сервера балансировки)
      - Node Exporter по нашим серверам (смотрю загруженность по железу)
      - Postgres Exporter (смотрю на состояние базы, на количество активных пользователей, смотрю аптайм)
      - Blackbox Exporter (смотрю, когда протухнут сертификаты и когда их нужно будет перевыпустить)
      - Kubernetes (смотрю на рестарт подов по нашим приложениям, смотрю на трафик, на нагрузку подов)

#### 6. Кто и как поднимал экспортеры и дашборды?
   - Я поднимал через юнит-сервисы все экспортеры и рисовал кастомные дашборды на основе официальных с сайта Grafana Lab:
      - Я создавал папку в Grafana, писал название дашборда, к примеру, если это Monitoring Nginx, то я наполнял дашборд метриками, которые снимал с помощью Nginx Exporter, который стоял на сервере с Nginx.
      - Как снимал?
         - Nginx Exporter был указан, как таргет (цель) в конфиге Prometheus и Prometheus скрейпал с него метрики
         - Далее я прописывал Prometheus Data Source в Grafana и получал доступ к метрикам
         - Рисовал виджеты с помощью формул на основе PromQL, какие-то кастомные, какие-то оригинальные копировал с дашборда Nginx

#### 7. Что именно рисовали? Какие виджеты? И какие метрики использовали? Перечисли типы метрик и какие золотые сигналы знаешь?
   - Рисовал виджеты [по золотым сигналам](https://github.com/lamjob1993/linux-monitoring/blob/main/tasks/prometheus/beginning/4.%20%D0%A7%D0%B5%D1%82%D1%8B%D1%80%D0%B5%20%D0%B7%D0%BE%D0%BB%D0%BE%D1%82%D1%8B%D1%85%20%D1%81%D0%B8%D0%B3%D0%BD%D0%B0%D0%BB%D0%B0%20(Four%20Golden%20Signals).md):
      - Соотношение HTTP успешных запросов к неуспешным
      - Состояние трафика сервера, Upstream, Network
      - Состояние: CPU Usage, Memory Usage
   - Использовал метрики в соответствии [с 4-мя типами метрик](https://github.com/lamjob1993/linux-monitoring/blob/main/tasks/prometheus/beginning/2.%20%D0%9C%D0%B5%D1%82%D1%80%D0%B8%D0%BA%D0%B8%20(%D0%A7%D0%B5%D1%82%D1%8B%D1%80%D0%B5%20%D0%BE%D1%81%D0%BD%D0%BE%D0%B2%D0%BD%D1%8B%D1%85%20%D1%82%D0%B8%D0%BF%D0%B0%20%D0%BC%D0%B5%D1%82%D1%80%D0%B8%D0%BA).md)

#### 8. Что такое виджет? Что такое дашборд?
   - **Дашборд** - это визуальная web-страница в формате JSON (это удобный GUI для визуализации метрик, как большой коллектор для виджетов на одной странице), которая подгружается в Grafana либо с официального сайта (JSON) либо же дашборд можно нарисовать вручную с нуля, наполнив кастомными виджетами.
   - **Виджет** - это визуальная составляющая дашборда (отдельный визуальный блок), виджеты рисуют с помощью метрик снимаемых с Data Source и описывают формулами, в моём случае языком PromQL.

#### 9. А как вывести переменные в Grafana?
   - Перехожу в раздел настроек своего дашборда, объявляю переменные в разделе Variables
   - К переменной внутри виджета обращаюсь через значок доллара $

#### 10. А в каком формате вы выгружали дашборды Grafana?
   - `JSON`

#### 11. А как у вас было реализовано версионирование дашбордов?
   - Мы экспортировали и подгружали `JSON` в `Bitbucket` и далее производили раскатку с помощью Teamcity/Jenkins (отвечаем только Jenkins или только Teamcity - двух инструментов быть не может)

#### 12. С Observability работали? Что это такое? 
   - [Ответ](https://github.com/lamjob1993/linux-monitoring/blob/main/navigation/introduction_monitoring/8.%20%D0%9D%D0%B0%D0%B1%D0%BB%D1%8E%D0%B4%D0%B0%D0%B5%D0%BC%D0%BE%D1%81%D1%82%D1%8C%20(Observability).md)

#### 13. А вот вы рассказали про конвейер и про адаптеры, а как это было реализовано?
   - [Ответ](https://github.com/lamjob1993/kubernetes-monitoring/blob/main/kubernetes/legend/1.%20Java%20%D0%B2%20%D0%BA%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B5.md)

#### 14. А вот вы рассказали про клиентов, а это кто такие?
   - [Ответ](https://github.com/lamjob1993/kubernetes-monitoring/blob/main/kubernetes/legend/4.%20%D0%9A%D0%BB%D0%B8%D0%B5%D0%BD%D1%82%D1%8B%20%D0%B1%D0%B0%D0%BD%D0%BA%D0%B0.md)

#### 15. Как действуете в случае аварийной ситуации, если падает приложение в Kubernetes (да по сути оно может упасть везде и на любой инфре, собеседующий может спросить абстрактно)?
   - [Ответ](https://github.com/lamjob1993/kubernetes-monitoring/blob/main/kubernetes/legend/5.%20%D0%90%D0%B2%D0%B0%D1%80%D0%B8%D0%B9%D0%BD%D0%B0%D1%8F%20%D1%81%D0%B8%D1%82%D1%83%D0%B0%D1%86%D0%B8%D1%8F%20%D0%BD%D0%B0%20%D0%9F%D0%A0%D0%9E%D0%94%D0%B5.md)

#### 16. А как вы автоматизировали рабочие процессы?
   - Docker Compose (писал Compose файлы на мониторинг серверов и приложений)
   - Ansible Roles (писал роли на экспортеры и на основные инструменты мониторинга (по сути любые конфиги, запуск юнит-сервисов, обновление ПО до 50-ти разных серверов от Debian до CentOS))
   - Bash (писал автоматизацию по разворачиванию бинарей через Unit-файлы: выдача прав, содать Unit, выставить автозапуск и т.д)

#### 17. С Git как обстоят дела? Как запушить изменения? Как скачать репозиторий? А какая система контроля версий была на проекте (в отделе)?
   - [Ответ](https://teletype.in/@lamjob/uhCHMD9zBeu)
   - Система контроля версий **Git**
      - Хостинг репозиториев **Bitbucket**

#### 18. Кто собирал встречу при инциденте?
   - Я собирал, я делал рассылку, я пинговал, как в ТГ, так и в корпоративном мессенджере. Далее мы шли по ссылке в Zoom.

#### 19. Как решали инцидент? Этапы решения?
   - Сначала я пошел смотреть на мониторинг и увидел (на дашборде Kubernetes), что был рестарт подов по нашему сервису, далее пошел смотреть на этот сервис в систему логирования OpenSearch (это аналог ELK), далее открываю JSON лог и вижу 502 или 503 ошибку (они будут в 90% случаев у вас) - ошибка на сервере (на каком сервере? - на Nginx), от нас поступает API запрос (что такое API? - [ответ](https://teletype.in/@lamjob/xnxhxKyYr93)) на Nginx-сервер, и сервер выдает ответ 502 или 503 - а это что такое:
      - `Ошибка 503 (Service Unavailable)` сообщает, что сервер временно недоступен и не может обработать запрос.
      - `Ошибка 502 (Bad Gateway)` означает, что сервер не получил ответ от другого сервера. Возможно, сервер, где хранится база, перестал работать или данные были случайно удалены.
   - Сначала решаю инцидент своими силами:
      - Провожу расследование (дашборд Grafana -> логи OpenSearch -> дёргаю за ручку API (о ручке ниже, ручка - это адрес `/path/to/api`) -> перезапуск сервиса в крайнем случае)
      - Проверяю API запрос и понимаю, что запрос неверный:
         - Почему неверный?
            - Я сверил с актуальной документацией API запрос с запросом API. И понял, что один из разработчиков совершил синтаксическую ошибку в коде при раскатке (через Teamcity) и адрес API побился
         - А какую роль играет API в вашем приложении?
            - У нас API поддерживает каждое Java приложение, это набор инструкций для обмена данными между приложениями
            - В нашем случае есть API, отвечающий за документы, за биллинг, за кредитный конвейер и т.д:
               - За каждый API можно дёрнуть и получить готовый JSON, либо восстановить работу приложения (отвечаем, что делали по инструкции в Confluence и собеседующий отстанет, но не выдавайте инструкцию явно, оставляем на последок)
         - А как дёргаете ручку? Каким запросом?
            - `GET`
         - А чем дёргаете? Каким инструментом (достаточно запомнить один)?
            - OpenLens (GUI для Kubernetes/K8S - визуализация кластера)
            - Postman (это программа для тестирования API (Application Programming Interface) и REST, упрощающая процесс отладки и тестирования кода)
            - Вручную из контейнера внутри пода (Kubernetes)
         - Если будет глобальный допрос - на какие кнопки нажимали (OpenLens, Postman и т.д)?
            - Отвечаем, что по инструкции в Confluence, потому что все приложения разные и у вас на конвейере их много и они микросервисные, и что на каждую API ручку свой запрос
   - Далее если все-таки я своими силами не справился, я с этими ошибками иду: или к разработке, или к DevOps по матрице эскалации (Матрица эскалации в сопровождении ПО — это заранее определённая последовательность действий, описывающая, кто из сотрудников, как и в каком порядке должен реагировать на инцидент.) или в аварийную конференцию.
      - Собираю всех заинтересованных разрабов и админов вместе в одной конфе в соответствии с матрицей.
      - Внимание! Данный подход описанный выше справедлив для любых инструментов, не только для API, старайтесь вплетать (внедрять) этот способ в ваш ответ собеседующему, тренируйтесь и всё получится. 

#### 20. Кстати, спросит собеседующий, а на каком языке у вас написан бэкенд?
   - Отвечаем Java и Kotlin, но в вашем сопровождении [только приложения на Java](https://github.com/lamjob1993/kubernetes-monitoring/tree/main/kubernetes/legend), из них 30 микросервисов (адаптертов), отвечающих за:
      - Документы
      - Биллинг
      - Прокси
      - Калькулятор (если кредиты)
      - И так далее

#### 21. В процессе вашего рассказа о компании или в любой неудобный момент вам могут задать пару вопросов:
   - **Например, расскажи какая была самая простая задача и какая самая сложная?**

       - **Простая**. Установка Node Exporter, ставил сам на нужные сервера (всегда стараемся отвечать подробно, чтобы из вас не выдавливал собеседующий доп.вопросы):
         - Как устанавливал?
            - Вручную на каждую VM тачку нашего отдела или Bash скриптом.
         - Как запускал скрипт?
            - Дал ему `chmod +x` права и запустил через `./bash-script.sh`
         - Расскажи про состав скрипта (эту формулу используем для любого bash скрипта на собеседовании)?
            - Скрипт представлял из себя тело, где был перебор последовательных команд:
               - Взять архив с бинарём из Nexus (наша файловая шара) по ссылке
               - Положить в `/tmp`, далее разархивировать его
               - Далее поставить `.bin` на автозапуск и запустить его
         - Как будешь ставить?
            - Пропишу юнит-файл на запуск Node Exporter в `systemd`
            - Перечитаю директорию systemd на новый юнит-файл Node Exporter командой `sudo systemctl daemon-reload`
            - Запущу Node Exporter командой `sudo systemctl start node.exporter`
            - Поставлю на автозапуск командой `sudo systemctl enable node.exporter`
            - Проверю Node Exporter на веб-морде на порту :9100
            - Пропишу в конфиге Prometheus таргет Node Exporter и проверю метрики экспортера (node_...) на веб-морде Prometheus на порту :9090
       - **Сложная**. Балансировка нагрузки на Nginx совместно с администраторами (если не знаем Nginx, то лучше ответить [вот так](https://github.com/lamjob1993/kubernetes-monitoring/blob/main/kubernetes/legend/5.%20%D0%90%D0%B2%D0%B0%D1%80%D0%B8%D0%B9%D0%BD%D0%B0%D1%8F%20%D1%81%D0%B8%D1%82%D1%83%D0%B0%D1%86%D0%B8%D1%8F%20%D0%BD%D0%B0%20%D0%9F%D0%A0%D0%9E%D0%94%D0%B5.md)):
         - Как балансировал (по какому алгоритму)? Что именно балансировал (какие серверы?)? Что такое round-robin (хотят услышать определение алгоритма)? Какие директивы и блоки использовал (хотят услышать названия директив и за что отвечали)? 
            - Я написал nginx.conf файл для балансировки по алгоритму round-robin (запросы распределяются между серверами последовательно: каждый новый запрос отправляется на следующий сервер в списке).
            - Далее диктуем собеседующему построчно:
            ```yaml
            # Определение группы серверов с именем "backend" для балансировки нагрузки
            upstream backend { # Блок (контейнер для серверов)
   
                   # Первый сервер в группе (будет получать запросы при балансировке)
                   server backend1.example.com; # Директива внутри блока upstream
                   
                   # Второй сервер в группе
                   server backend2.example.com; # Директива внутри блока upstream
                   
                   # Третий сервер в группе
                server backend3.example.com; # Директива внутри блока upstream
            }
            
            # Конфигурация виртуального сервера (блока server)
            server { # Блок виртуального хоста
                # Указание порта (80) для прослушивания входящих соединений
                listen 80; # Директива
                
                # Имя сервера, для которого обрабатываются запросы
                server_name app.example.com; # Директива
                
                # Блок location для обработки запросов по пути "/" (все запросы)
                location / { # Блок для обработки URL-путей
                    # Перенаправление всех запросов на группу серверов "backend"
                    proxy_pass http://backend; # Директива
                }
            }
            ```
         - А также можете продиктовать следующее. Расшифровка конфига Nginx:            
            1. `upstream` - определяет группу серверов для балансировки нагрузки
            2. По умолчанию используется алгоритм Round Robin (по очереди)
            3. `server` блок обрабатывает запросы для указанного домена
            4. `location /` - обрабатывает все пути, начинающиеся с /
            5. `proxy_pass` - перенаправляет запросы на указанную группу серверов
            6. Эта конфигурация создает простой балансировщик нагрузки, который равномерно распределяет запросы между тремя бэкенд-серверами.

         - Какие директивы использовал и какие есть?
            - Указаны в конфиге выше
            - Кто писал конфиг?
               - Я писал конфиг
            - Как запускал-применял конфиг?
               - Перечитывал конфиг-файл Nginx командой `sudo systemctl reload nginx `
            - Как проверял запуск?
               - Командой `sudo systemctl status nginx `
         - Что именно балансировал (какие серверы?) и почему?
            - Я балансировал серверы Mimir, их было три, это горизонатальное масштабирование Prometheus с помощью Nginx
            - Почему Mimir, а не федерация?
               - Нам не хватало [производительности федерации](https://github.com/lamjob1993/linux-monitoring/tree/main/tasks/prometheus_federate), поэтому решили перейти на Mimir

---
